\section{Simplifications for the original problem}

\noindent\textbf{Notations.} Let $\e_j$ be a standard unit vector in $\R^p$ with $1$ in the $j$th coordinate and $0$ in all other coordinates. And $\e$ is the vector with all entries equal to $1$.

The original optimization is as follows
\begin{align*}
     & \underset{\bOmega}{\min}~\norm{\bOmega}_{1},
    \\
     & \text{s.t.}~ \abs{\bSigma_n\bOmega - \I}_{\infty} \leq \lambda_n,
\end{align*}
where $\bOmega$ and $\bSigma_n$ are the $p\times p$ precision matrix and sample covariance matrix, respectively. The linear program can be decomposed into $p$ vector minimization problems. For $1\leq j \leq p$, the $j$th linear program is as follows.
\begin{equation}\label{original:opt}
    \begin{aligned}
         & \underset{\bbeta\in\R^p}{\min}~\norm{\bbeta}_1,
        \\
         & \text{s.t.}~ \abs{\bSigma_n\bbeta - \e_j}_{\infty} \leq \lambda_n.
    \end{aligned}
\end{equation}

Then considering the $j$th optimization, we rewrite the variable $\bbeta$ as $\bbeta = \bbeta_{+} - \bbeta_{-}$ with $\bbeta_{+}, \bbeta_{-} \geq 0$. Thus the $\ell_1$ norm can be written as
\begin{align}
    \norm{\bbeta}_1
    = \e\trans\bbeta_{+} + \e\trans\bbeta_{-}, \label{opt:part:1}
\end{align}
because there can be only one part not zero (e.g. $x_{+}=\max\{x,0\},~x_{-}=\max\{-x,0\}$ satisfying $x=x_{+}-x_{-}$). Moreover, utilizing the relaxation variable, the constraint can be written as
\begin{align}
    \left\{
    \begin{aligned}
         & \bSigma_n\bbeta - \e_j + \w_{+} = \lambda_n \e,
        \\
         & \w_{+} + \w_{-} = 2\lambda_n\e,
        \\
         & \w_{+}, \w_{-} \geq 0.
    \end{aligned}
    \right. \label{opt:part:2}
\end{align}
Thus combining \eqref{original:opt}, \eqref{opt:part:1} and \eqref{opt:part:2}, we can get the reformatted optimization as follows
\begin{equation}\label{opt:1}
    \begin{aligned}
         & \min~ \e\trans\bbeta_{+} + \e\trans\bbeta_{-}
        \\
         & \text{s.t.}
        \\
         & \begin{aligned}
             & \bSigma_n\bbeta_{+} - \bSigma\bbeta_{-} + \w_{+} = \lambda_n\e + \e_j,
            \\
             & \w_{+} + \w_{-} = 2\lambda_n \e,
            \\
             & \bbeta_{+}, \, \bbeta_{-}, \, \w_{+}, \, \w_{-} \geq 0,
        \end{aligned}
    \end{aligned}
\end{equation}
where all of the four vectors $\bbeta_{+},\bbeta_{-},\w_{+},\w_{-}$ are $p$-dimensional. Then we set
\begin{align*}
     & \c = \left(\e\trans, \e\trans, \0_p\trans, \0_p\trans\right)\trans\in \R^{4p},
    \\
     & \x = \left(\bbeta_{+}\trans, \bbeta_{-}\trans, \w_{+}\trans, \w_{-}\trans\right)\trans \in \R^{4p},
    \\
     & \b = \left(\lambda_n\e\trans+\e_j\trans, 2\lambda_n \e\trans\right)\trans \in \R^{2p},
\end{align*}
and the constraint matrix as
\begin{align*}
    \A =
    \left[
        \begin{array}{cccc}
            \bSigma_n      & -\bSigma_n     & \I_p & \0_{p\times p} \\
            \0_{p\times p} & \0_{p\times p} & \I_p & \I_p
        \end{array}
        \right] \in \R^{2p\times 4p}.
\end{align*}

With the above definition, the optimization \eqref{opt:1} can be written as
\begin{align*}
     & \underset{\x\in\R^{4p}}{\min}~ \c\trans\x
    \\
     & \begin{aligned}
        \text{s.t.}~ & \A\x = \b
        \\
                     & \x \geq \0_{4p}
    \end{aligned}
\end{align*}

\section{Implementations for ABIP}

\begin{align*}
    &\Q =
    \left[
        \begin{array}{cccc}
            \0              & \A             & -\b       & \bar{\b}  \\
            -\A\trans       & \0             & \c        & -\bar{\c} \\
            \b\trans        & -\c\trans      & 0        & \bar{z}  \\
            -\bar{\b}\trans & \bar{\c}\trans & -\bar{z} & 0
        \end{array}
        \right],
    \quad
    \u = \left[
        \begin{array}{c}
            \y   \\
            \x   \\
            \tau \\
            \theta
        \end{array}
        \right],
    \quad
    \v = \left[
        \begin{array}{c}
            \r     \\
            \s     \\
            \kappa \\
            \xi
        \end{array}
        \right] 
        \\
        &\bar{\b} = \b - \A\e, \quad 
        \bar{\c} = \c - \e, \quad 
        \bar{z} = \c\trans\e + 1.
\end{align*}

\subsection{Inner iteration} 

In the inner iteration with fixed $k$, we follow the update rule as follows (from $i$ to $i+1$). 

\noindent\underline{1. Update $\wt{\u}_{i+1}$:} 
\begin{align*}
    \wt{\u}_{i+1}^k = (\I + \Q)^{-1} (\u_i^k + \v_i^k). 
\end{align*}
\noindent\underline{2. Update $\u_{i+1}^k$:}
\begin{align*}
    &\y_{i+1}^k = \wt{\y}_{i+1}^k 
    \\ 
    &\x_{i+1}^k = 
    \frac{1}{2}\left[
        (\wt{\x}_{i+1}^k - \s_i^k) + 
        \sqrt{(\wt{\x}_{i+1}^k - \s_i^k)\circ(\wt{\x}_{i+1}^k - \s_i^k) + \frac{4\mu^k}{\beta}}
    \right], 
    \\ 
    &\tau_{i+1}^k = 
    \frac{1}{2}\left[
        (\wt{\tau}_{i+1}^k - \kappa_i^k) 
        + \sqrt{(\wt{\tau}_{i+1}^k - \kappa_i^k)\circ(\wt{\tau}_{i+1}^k - \kappa_i^k) + \frac{4\mu^k}{\beta}}
    \right], 
    \\ 
    &\theta_{i+1}^k = \wt{\theta}_{i+1}^k,
\end{align*}
\noindent\underline{3. Update $\v_{i+1}^k$:}
\begin{align*}
\v_{i+1}^k = \v_i^k - \wt{\u}_{i+1}^k + \u_{i+1}^k 
\end{align*}

\subsection{Outer iteration}

In the outer iteration, the index $i+1$ reset to zero and update $k$ to $k+1$. The computation parameter is updated as follows. 
\begin{align*}
    \mu^{k+1} = \gamma \cdot \mu^k.
\end{align*}
Moreover, we need to reset the initialization as follows. 
\begin{align*}
    &\r_0^{k+1} = \0, 
    \\
    &\xi_0^{k+1} = -n - 1, ~\text{here the $n$ is the column number of $\A$}, 
    \\ 
    &(\y_0^{k+1},\x_0^{k+1}, \s_0^{k+1}, \tau_0^{k+1}, \kappa_0^{k+1}, \theta_0^{k+1}) 
    = \sqrt{\gamma}\cdot 
    (\y_{i+1}^k,\x_{i+1}^k, \s_{i+1}^k, \tau_{i+1}^k, \kappa_{i+1}^k, \theta_{i+1}^k)
\end{align*}

\subsection{The inverse of I + Q}